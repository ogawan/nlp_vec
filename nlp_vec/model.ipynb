{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "25688049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbb89c",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Handling of stopwords when training word2vec. \n",
    "* [stackoverflow](https://stackoverflow.com/questions/34721984/stopword-removing-when-using-the-word2vec)\n",
    "\n",
    "* [Removing plural,ed, ing](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5ff6b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"This guideline emphasizes considerations of both safety and quality risk management in establishing levels of mutagenic impurities that are expected to pose negligible carcinogenic risk. It outlines recommendations for assessment and control of mutagenic impurities that reside or are reasonably expected to reside in final drug substance or product, taking into consideration the intended conditions of human use.\"\n",
    "# Prepare paragraph\n",
    "\n",
    "sentences = doc.split(\".\")\n",
    "tokens =[]\n",
    "tokens_cleaned = []\n",
    "for sentence in sentences:\n",
    "    \n",
    "    split_sentence = sentence.split(\" \")\n",
    "\n",
    "    # Remove empty string\n",
    "    split_sentence  = list(filter(None, split_sentence ))\n",
    "    tokens = tokens + split_sentence\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "#ls = LancasterStemmer()\n",
    "for token in tokens:\n",
    "    token = wnl.lemmatize(token)\n",
    "    #token = ls.stem(token)\n",
    "    token = token.lower()\n",
    "    token = re.sub(' +',' ',token)\n",
    "    token = token.strip()\n",
    "    tokens_cleaned.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "13d968ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of words\n",
    "counts = dict(Counter(tokens_cleaned))\n",
    "sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "word2index = dict([(my_tuple[0],idx) for idx,my_tuple in enumerate(sorted_counts,1)])\n",
    "inverted_word2index = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4155b8d7-0ef1-42ac-bdb6-18bf629cdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(seq, n=5):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ------------\n",
    "        seq: list\n",
    "            Sentence as lit of words\n",
    "        n:integer\n",
    "            the window size\n",
    "    yield:\n",
    "    -----------\n",
    "        result: generator object\n",
    "            Sliding windows\n",
    "        \n",
    "    \"\"\"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result\n",
    "\n",
    "def sample_examples(docs,max_window_size,n_windows,do_negs):\n",
    "    '''generate target,context pairs and negative examples'''\n",
    "    windows = []\n",
    "    for i,doc in enumerate(docs):\n",
    "        window_size = int(np.random.choice(range(1,max_window_size+1),1)) # dynamic window\n",
    "        to_append = list(get_windows(doc,2*window_size+1))\n",
    "        to_append = [list((i,) + elt) for elt in to_append] # convert to list to support del and pop\n",
    "        windows.append(to_append)\n",
    "\n",
    "    windows = [elt for sublist in windows for elt in sublist] # flatten\n",
    "    random_idxs = np.random.choice(range(len(windows)),size=n_windows,replace=False)\n",
    "    windows = [windows[idx] for idx in random_idxs]\n",
    "    \n",
    "    if do_negs:\n",
    "        all_negs = list(np.random.choice(token_ints,size=n_negs*len(windows),p=neg_distr))\n",
    "        to_return = windows,all_negs\n",
    "    else:\n",
    "        to_return = windows\n",
    "    \n",
    "    return to_return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
